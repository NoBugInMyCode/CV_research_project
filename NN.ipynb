{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-08T09:50:33.800918500Z",
     "start_time": "2023-09-08T09:50:33.783920300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class ActivationDoesNotExist(Exception):\n",
    "    # Valid activations are sigmoid, tanh, and relu, provided as a string\n",
    "    pass\n",
    "\n",
    "class InputDimensionNotCorrect(Exception):\n",
    "    # Need to specify input dimension, i.e. input shape into the first layer\n",
    "    pass\n",
    "\n",
    "class LossFunctionNotDefined(Exception):\n",
    "    # Loss function in cost() method not defined\n",
    "    pass\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, inputDimension, units, activation='', randomMultiplier=0.01):\n",
    "        self.weights, self.bias = self.initialize(inputDimension, units, randomMultiplier)\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = activation\n",
    "            self.activationForward = self.sigmoid\n",
    "            self.activationBackward = self.sigmoidGrad\n",
    "        elif activation == 'relu':\n",
    "            self.activation = activation\n",
    "            self.activationForward = self.relu\n",
    "            self.activationBackward = self.reluGrad\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = activation\n",
    "            self.activationForward = self.tanh\n",
    "            self.activationBackward = self.tanhGrad\n",
    "        elif activation != '':\n",
    "            raise ActivationDoesNotExist\n",
    "        else:\n",
    "            self.activation = 'none'\n",
    "            self.activationForward = self.linear\n",
    "            self.activationBackward = self.linear\n",
    "\n",
    "    def initialize(self, nx, nh, randomMultiplier):\n",
    "        weights = randomMultiplier * np.random.randn(nh, nx)\n",
    "        bias = np.zeros([nh, 1])\n",
    "        return weights, bias\n",
    "\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        A = 1 / (1 + np.exp(-Z))\n",
    "        return A\n",
    "\n",
    "    def sigmoidGrad(self, dA):\n",
    "        s = 1 / (1 + np.exp(-self.prevZ))\n",
    "        dZ = dA * s * (1 - s)\n",
    "        return dZ\n",
    "\n",
    "\n",
    "    def relu(self, Z):\n",
    "        A = np.maximum(0, Z)\n",
    "        return A\n",
    "\n",
    "    def reluGrad(self, dA):\n",
    "        s = np.maximum(0, self.prevZ)\n",
    "        dZ = (s>0) * 1 * dA\n",
    "        return dZ\n",
    "\n",
    "\n",
    "    def tanh(self, Z):\n",
    "        A = np.tanh(Z)\n",
    "        return A\n",
    "\n",
    "    def tanhGrad(self, dA):\n",
    "        s = np.tanh(self.prevZ)\n",
    "        dZ = (1 - s**2) * dA\n",
    "        return dZ\n",
    "\n",
    "\n",
    "    def linear(self, Z):\n",
    "        return Z\n",
    "\n",
    "    def forward(self, A):\n",
    "        Z = np.dot(self.weights, A) + self.bias\n",
    "        self.prevZ = Z\n",
    "        self.prevA = A\n",
    "        A = self.activationForward(Z)\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward pass through layer\n",
    "          dA: previous gradient\n",
    "        \"\"\"\n",
    "        dZ = self.activationBackward(dA)\n",
    "        m = self.prevA.shape[1]\n",
    "        self.dW = 1 / m * np.dot(dZ, self.prevA.T)\n",
    "        self.db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        prevdA = np.dot(self.weights.T, dZ)\n",
    "        return prevdA\n",
    "\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        self.weights = self.weights - learning_rate * self.dW\n",
    "        self.bias = self.bias - learning_rate * self.db\n",
    "\n",
    "    def outputDimension(self):\n",
    "        return len(self.bias)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        act = 'none' if self.activation == '' else self.activation\n",
    "        return f'Dense layer (nx={self.weights.shape[1]}, nh={self.weights.shape[0]}, activation={act})'\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, loss='cross-entropy', randomMultiplier = 0.01):\n",
    "        self.layers=[]\n",
    "        self.randomMultiplier = randomMultiplier\n",
    "        if loss=='cross-entropy':\n",
    "            self.lossFunction = self.crossEntropyLoss\n",
    "            self.lossBackward = self.crossEntropyLossGrad\n",
    "        elif loss=='mean-square-error':\n",
    "            self.lossFunction = self.meanSquareError\n",
    "            self.lossBackward = self.meanSquareErrorGrad\n",
    "        else:\n",
    "            raise LossFunctionNotDefined\n",
    "        self.loss=loss\n",
    "\n",
    "\n",
    "    def addLayer(self, inputDimension=None, units=1, activation=''):\n",
    "        if (inputDimension is None):\n",
    "            if (len(self.layers)==0):\n",
    "                raise InputDimensionNotCorrect\n",
    "            inputDimension=self.layers[-1].outputDimension()\n",
    "        layer = DenseLayer(inputDimension, units, activation, randomMultiplier= self.randomMultiplier)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def crossEntropyLoss(self, Y, A, epsilon=1e-15):\n",
    "        m = Y.shape[1]\n",
    "        loss = -1 * (Y * np.log(A + epsilon) + (1 - Y) * np.log(1 - A + epsilon))\n",
    "        cost = 1 / m * np.sum(loss)\n",
    "        return np.squeeze(cost)\n",
    "\n",
    "    def crossEntropyLossGrad(self, Y, A):\n",
    "        dA = -(np.divide(Y, A) - np.divide(1 - Y, 1 - A))\n",
    "        return dA\n",
    "\n",
    "\n",
    "    def meanSquareError(self, Y, A):\n",
    "        loss = np.square(Y - A)\n",
    "        m = Y.shape[1]\n",
    "        cost = 1 / m * np.sum(loss)\n",
    "        return np.squeeze(cost)\n",
    "\n",
    "    def meanSquareErrorGrad(self, Y, A):\n",
    "        dA = -2 * (Y - A)\n",
    "        return dA\n",
    "\n",
    "\n",
    "    def cost(self, Y, A):\n",
    "        return self.lossFunction(Y, A)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = np.copy(X)\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def backward(self, A, Y):\n",
    "        dA = self.lossBackward(Y, A)\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "\n",
    "\n",
    "    def update(self, learning_rate=0.01):\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        layrepr = ['  ' + str(ix+1)+' -> ' + str(x) for ix, x in enumerate(self.layers)]\n",
    "        return '[\\n' + '\\n'.join(layrepr) + '\\n]'\n",
    "\n",
    "\n",
    "    def numberOfParameters(self):\n",
    "        n = 0\n",
    "        for layer in self.layers:\n",
    "            n += np.size(layer.weights) + len(layer.bias)\n",
    "        print(f'There are {n} trainable parameters in the model.')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "            0      1      2       3      4      5  6\n0        7.20  12.30  18.87   42.02   6.14  25.52  0\n1       11.36  10.46  21.99   22.59  22.09  64.97  1\n2        8.12   8.40   6.70   12.67  13.92  14.20  0\n3       28.96  11.06  17.60   40.36  23.71  39.91  1\n4       36.72  35.99  33.29   32.10  12.10  21.12  1\n...       ...    ...    ...     ...    ...    ... ..\n999995  32.91  13.67  35.62  119.69  13.81  52.39  1\n999996  15.26  21.69  20.63   45.90  25.07  45.23  1\n999997  38.96   8.38  34.08   12.14  10.56  14.09  0\n999998  42.02  80.13  19.48   23.22  14.88  29.38  1\n999999  37.79  22.94  24.42   24.53  23.76  40.80  1\n\n[1000000 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.20</td>\n      <td>12.30</td>\n      <td>18.87</td>\n      <td>42.02</td>\n      <td>6.14</td>\n      <td>25.52</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11.36</td>\n      <td>10.46</td>\n      <td>21.99</td>\n      <td>22.59</td>\n      <td>22.09</td>\n      <td>64.97</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.12</td>\n      <td>8.40</td>\n      <td>6.70</td>\n      <td>12.67</td>\n      <td>13.92</td>\n      <td>14.20</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>28.96</td>\n      <td>11.06</td>\n      <td>17.60</td>\n      <td>40.36</td>\n      <td>23.71</td>\n      <td>39.91</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>36.72</td>\n      <td>35.99</td>\n      <td>33.29</td>\n      <td>32.10</td>\n      <td>12.10</td>\n      <td>21.12</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>999995</th>\n      <td>32.91</td>\n      <td>13.67</td>\n      <td>35.62</td>\n      <td>119.69</td>\n      <td>13.81</td>\n      <td>52.39</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>999996</th>\n      <td>15.26</td>\n      <td>21.69</td>\n      <td>20.63</td>\n      <td>45.90</td>\n      <td>25.07</td>\n      <td>45.23</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>999997</th>\n      <td>38.96</td>\n      <td>8.38</td>\n      <td>34.08</td>\n      <td>12.14</td>\n      <td>10.56</td>\n      <td>14.09</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>999998</th>\n      <td>42.02</td>\n      <td>80.13</td>\n      <td>19.48</td>\n      <td>23.22</td>\n      <td>14.88</td>\n      <td>29.38</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>999999</th>\n      <td>37.79</td>\n      <td>22.94</td>\n      <td>24.42</td>\n      <td>24.53</td>\n      <td>23.76</td>\n      <td>40.80</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000000 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读取数据\n",
    "data = pd.read_csv('CW_project4.txt', delim_whitespace=True, header=None)\n",
    "data\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T09:50:35.727688500Z",
     "start_time": "2023-09-08T09:50:33.789919700Z"
    }
   },
   "id": "bcfe6a7a8ddc1cb7"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Cost: 0.24818240839834166\n",
      "Epoch 100, Cost: 0.2231535684643292\n",
      "Epoch 200, Cost: 0.21824561328199737\n",
      "Epoch 300, Cost: 0.21345806380323795\n",
      "Epoch 400, Cost: 0.20873964564479805\n",
      "Epoch 500, Cost: 0.20401972941377308\n",
      "Epoch 600, Cost: 0.19922933868905351\n",
      "Epoch 700, Cost: 0.19429848540147998\n",
      "Epoch 800, Cost: 0.18917649961804137\n",
      "Epoch 900, Cost: 0.1838388472344335\n",
      "Sample Input: [[14.92 46.64 95.21 28.99 27.13 21.58]]\n",
      "Sample Output: [[1.]]\n",
      "Predicted Output: [[0.74063588]]\n",
      "Accuracy: 78.60%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1,1) \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.T.astype(np.float64)\n",
    "y_test = y_test.T.astype(np.float64)\n",
    "\n",
    "\n",
    "nn = NeuralNetwork(loss='mean-square-error')\n",
    "\n",
    "\n",
    "nn.addLayer(inputDimension=X_train.shape[0], units=256, activation='relu')\n",
    "nn.addLayer(units=128, activation='relu')\n",
    "nn.addLayer(units=64, activation='relu')\n",
    "nn.addLayer(units=1, activation='sigmoid')\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 2000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    A = nn.forward(X_train)\n",
    "    nn.backward(A, y_train)\n",
    "    nn.update(learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "        cost = nn.cost(y_train, A)\n",
    "        print(f'Epoch {epoch}, Cost: {cost}')\n",
    "\n",
    "\n",
    "sample_input = X_test[:,0].reshape(-1, 1)\n",
    "sample_output = y_test[:,0].reshape(-1, 1)\n",
    "predicted_output = nn.forward(sample_input)\n",
    "\n",
    "\n",
    "A_test = nn.forward(X_test)\n",
    "predictions = (A_test > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T09:55:08.692724300Z",
     "start_time": "2023-09-08T09:50:35.728692400Z"
    }
   },
   "id": "cc0326821e3db9a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
